{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Data Collection\n",
    "\n",
    "Fetch all pages transcluding `{{Playlist}}` from utaite.wiki via pywikibot.\n",
    "\n",
    "**Output:** Raw wikitext files in `data/raw/` + `data/raw/manifest.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/akkun/utaitewiki-songlist-migration\n",
      "Data output:  /home/akkun/utaitewiki-songlist-migration/data/raw\n",
      "PWB config:   /home/akkun/utaitewiki-songlist-migration/src/pwb_config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Project root\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure pywikibot BEFORE importing it\n",
    "os.environ[\"PYWIKIBOT_DIR\"] = str(PROJECT_ROOT / \"src\" / \"pwb_config\")\n",
    "\n",
    "# Add src to path so pwb can find the family file\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\" / \"pwb_config\"))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data output:  {DATA_RAW}\")\n",
    "print(f\"PWB config:   {os.environ['PYWIKIBOT_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: utaite.wiki\n",
      "\n",
      "Note: We're running in read-only mode (no login required).\n",
      "The wiki's public API allows fetching page content without authentication.\n"
     ]
    }
   ],
   "source": [
    "import pywikibot\n",
    "from pywikibot import pagegenerators\n",
    "\n",
    "# Connect to utaite.wiki\n",
    "site = pywikibot.Site(\"en\", \"utaitewiki\")\n",
    "\n",
    "print(f\"Connected to: {site.hostname()}\")\n",
    "print(\"\\nNote: We're running in read-only mode (no login required).\")\n",
    "print(\"The wiki's public API allows fetching page content without authentication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get All Pages Transcluding `{{Playlist}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pages transcluding Template:Playlist...\n",
      "Namespaces: 0 (Main), 14 (Category)\n",
      "Filtering out redirects...\n",
      "\n",
      "Found 925 non-redirect pages transcluding Template:Playlist\n",
      "  - ns=0 (Main): 891\n",
      "  - ns=14 (Category): 34\n"
     ]
    }
   ],
   "source": [
    "# Fetch all pages that transclude Template:Playlist\n",
    "# Namespaces: 0 (main) and 14 (category)\n",
    "template_page = pywikibot.Page(site, \"Template:Playlist\")\n",
    "\n",
    "print(\"Fetching pages transcluding Template:Playlist...\")\n",
    "print(\"Namespaces: 0 (Main), 14 (Category)\")\n",
    "print(\"Filtering out redirects...\")\n",
    "print()\n",
    "\n",
    "all_pages = []\n",
    "for page in template_page.embeddedin(namespaces=[0, 14], filter_redirects=False):\n",
    "    if page.isRedirectPage():\n",
    "        continue\n",
    "    all_pages.append(page)\n",
    "\n",
    "print(f\"Found {len(all_pages)} non-redirect pages transcluding Template:Playlist\")\n",
    "print(f\"  - ns=0 (Main): {sum(1 for p in all_pages if p.namespace() == 0)}\")\n",
    "print(f\"  - ns=14 (Category): {sum(1 for p in all_pages if p.namespace() == 14)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Exclusion Set (Song Article Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'Utattemita Songs': 35 pages to exclude\n",
      "Category 'Famous Utattemita Songs': 667 pages to exclude\n",
      "\n",
      "Total exclusion set: 667 unique pages\n"
     ]
    }
   ],
   "source": [
    "# Exclude pages in \"Utattemita Songs\" and \"Famous Utattemita Songs\" categories\n",
    "# These are individual song articles, not artist playlists\n",
    "\n",
    "exclude_categories = [\n",
    "    \"Utattemita Songs\",\n",
    "    \"Famous Utattemita Songs\",\n",
    "]\n",
    "\n",
    "excluded_pages = set()\n",
    "for cat_name in exclude_categories:\n",
    "    cat = pywikibot.Category(site, f\"Category:{cat_name}\")\n",
    "    members = list(cat.members(namespaces=[0]))\n",
    "    excluded_pages.update(p.title() for p in members)\n",
    "    print(f\"Category '{cat_name}': {len(members)} pages to exclude\")\n",
    "\n",
    "print(f\"\\nTotal exclusion set: {len(excluded_pages)} unique pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter: 925 pages\n",
      "Excluded:      0 song article pages\n",
      "After filter:  925 pages to collect\n"
     ]
    }
   ],
   "source": [
    "# Apply exclusion filter\n",
    "filtered_pages = [p for p in all_pages if p.title() not in excluded_pages]\n",
    "excluded_count = len(all_pages) - len(filtered_pages)\n",
    "\n",
    "print(f\"Before filter: {len(all_pages)} pages\")\n",
    "print(f\"Excluded:      {excluded_count} song article pages\")\n",
    "print(f\"After filter:  {len(filtered_pages)} pages to collect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Associate Subpages to Root Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'Ameno Kosame' -> 'Ameno Kosame'\n",
      "  'Mafumafu/Songs/Cover' -> 'Mafumafu'\n",
      "  'Mafumafu/Songs/Original' -> 'Mafumafu'\n",
      "  'Mafumafu/Songs/Privated' -> 'Mafumafu'\n",
      "  'Mafumafu/Discography/2021-2025' -> 'Mafumafu'\n",
      "  'Category:Some Category' -> 'Category:Some Category'\n"
     ]
    }
   ],
   "source": [
    "def get_root_artist(page_title: str) -> str:\n",
    "    \"\"\"Extract root artist name from a page title.\n",
    "    \n",
    "    Examples:\n",
    "        'Ameno Kosame' -> 'Ameno Kosame'\n",
    "        'Mafumafu/Songs/Cover' -> 'Mafumafu'\n",
    "        'Mafumafu/Songs/Original' -> 'Mafumafu'\n",
    "        'Mafumafu/Discography/2021-2025' -> 'Mafumafu'\n",
    "        'Category:Some Category' -> 'Category:Some Category'\n",
    "    \"\"\"\n",
    "    # Don't process category pages\n",
    "    if page_title.startswith(\"Category:\"):\n",
    "        return page_title\n",
    "    \n",
    "    # Split on '/' and take the first part\n",
    "    return page_title.split(\"/\")[0]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_cases = [\n",
    "    \"Ameno Kosame\",\n",
    "    \"Mafumafu/Songs/Cover\",\n",
    "    \"Mafumafu/Songs/Original\",\n",
    "    \"Mafumafu/Songs/Privated\",\n",
    "    \"Mafumafu/Discography/2021-2025\",\n",
    "    \"Category:Some Category\",\n",
    "]\n",
    "for tc in test_cases:\n",
    "    print(f\"  '{tc}' -> '{get_root_artist(tc)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fetch Wikitext and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mafumafu__Songs__Cover\n",
      "Category_SIXFONIA\n"
     ]
    }
   ],
   "source": [
    "def sanitize_filename(title: str) -> str:\n",
    "    \"\"\"Convert a page title to a safe filename.\n",
    "    \n",
    "    Replaces characters that are invalid in filenames.\n",
    "    \"\"\"\n",
    "    # Replace / with __\n",
    "    safe = title.replace(\"/\", \"__\")\n",
    "    # Replace other problematic characters\n",
    "    safe = re.sub(r'[<>:\"\\\\|?*]', '_', safe)\n",
    "    return safe\n",
    "\n",
    "\n",
    "# Test\n",
    "print(sanitize_filename(\"Mafumafu/Songs/Cover\"))  # Mafumafu__Songs__Cover\n",
    "print(sanitize_filename(\"Category:SIXFONIA\"))       # Category_SIXFONIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching wikitext for 925 pages...\n",
      "============================================================\n",
      "  [50/925] Fetched: Hachi\n",
      "  [100/925] Fetched: 000\n",
      "  [150/925] Fetched: Enn\n",
      "  [200/925] Fetched: Jess Nuno\n",
      "  [250/925] Fetched: Rena\n",
      "  [300/925] Fetched: Nanamori\n",
      "  [350/925] Fetched: Rimokon (NND)\n",
      "  [400/925] Fetched: SymaG\n",
      "  [450/925] Fetched: Kurohina\n",
      "  [500/925] Fetched: HAKURO\n",
      "  [550/925] Fetched: Kurousagi Uru\n",
      "  [600/925] Fetched: Rin Rin\n",
      "  [650/925] Fetched: Rere\n",
      "  [700/925] Fetched: Root/Songs\n",
      "  [750/925] Fetched: Meloa\n",
      "  [800/925] Fetched: Pokota/Songs\n",
      "  [850/925] Fetched: For The More\n",
      "  [900/925] Fetched: Category:Circle of Friends\n",
      "  [925/925] Fetched: Category:Zessei Bijin!\n",
      "============================================================\n",
      "Done! Collected 925 pages.\n"
     ]
    }
   ],
   "source": [
    "# Fetch wikitext for all filtered pages and save to data/raw/\n",
    "manifest = {\n",
    "    \"collection_date\": datetime.now().isoformat(),\n",
    "    \"total_pages_found\": len(all_pages),\n",
    "    \"excluded_count\": excluded_count,\n",
    "    \"collected_count\": 0,\n",
    "    \"errors\": [],\n",
    "    \"pages\": [],\n",
    "}\n",
    "\n",
    "print(f\"Fetching wikitext for {len(filtered_pages)} pages...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, page in enumerate(filtered_pages, 1):\n",
    "    title = page.title()\n",
    "    root_artist = get_root_artist(title)\n",
    "    filename = sanitize_filename(title) + \".txt\"\n",
    "    filepath = DATA_RAW / filename\n",
    "    \n",
    "    try:\n",
    "        # Fetch raw wikitext (latest revision)\n",
    "        wikitext = page.text\n",
    "        \n",
    "        # Save to disk\n",
    "        filepath.write_text(wikitext, encoding=\"utf-8\")\n",
    "        \n",
    "        # Add to manifest\n",
    "        manifest[\"pages\"].append({\n",
    "            \"title\": title,\n",
    "            \"root_artist\": root_artist,\n",
    "            \"namespace\": page.namespace().id,\n",
    "            \"filename\": filename,\n",
    "            \"size_bytes\": len(wikitext.encode(\"utf-8\")),\n",
    "            \"is_subpage\": \"/\" in title and not title.startswith(\"Category:\"),\n",
    "        })\n",
    "        \n",
    "        if i % 50 == 0 or i == len(filtered_pages):\n",
    "            print(f\"  [{i}/{len(filtered_pages)}] Fetched: {title}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        manifest[\"errors\"].append({\"title\": title, \"error\": str(e)})\n",
    "        print(f\"  [ERROR] {title}: {e}\")\n",
    "\n",
    "manifest[\"collected_count\"] = len(manifest[\"pages\"])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Done! Collected {manifest['collected_count']} pages.\")\n",
    "if manifest[\"errors\"]:\n",
    "    print(f\"Errors: {len(manifest['errors'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest saved to: /home/akkun/utaitewiki-songlist-migration/data/raw/manifest.json\n",
      "Total files in data/raw/: 925\n"
     ]
    }
   ],
   "source": [
    "# Save manifest\n",
    "manifest_path = DATA_RAW / \"manifest.json\"\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Manifest saved to: {manifest_path}\")\n",
    "print(f\"Total files in data/raw/: {len(list(DATA_RAW.glob('*.txt')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Collection Summary ===\n",
      "Total pages collected: 925\n",
      "Unique root artists:  911\n",
      "Subpages:             128\n",
      "Main pages:           797\n",
      "\n",
      "Namespace breakdown:\n",
      "  Main: 891\n",
      "  Category: 34\n",
      "\n",
      "Total wikitext size: 11.2 MB\n",
      "Average page size:   12.4 KB\n",
      "Largest page:        Hanatan/Songs (91.7 KB)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load manifest into a DataFrame for quick analysis\n",
    "df = pd.DataFrame(manifest[\"pages\"])\n",
    "\n",
    "print(\"=== Collection Summary ===\")\n",
    "print(f\"Total pages collected: {len(df)}\")\n",
    "print(f\"Unique root artists:  {df['root_artist'].nunique()}\")\n",
    "print(f\"Subpages:             {df['is_subpage'].sum()}\")\n",
    "print(f\"Main pages:           {(~df['is_subpage']).sum()}\")\n",
    "print(f\"\")\n",
    "print(f\"Namespace breakdown:\")\n",
    "for ns, count in df['namespace'].value_counts().items():\n",
    "    ns_name = {0: 'Main', 14: 'Category'}.get(ns, f'ns={ns}')\n",
    "    print(f\"  {ns_name}: {count}\")\n",
    "print(f\"\")\n",
    "print(f\"Total wikitext size: {df['size_bytes'].sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Average page size:   {df['size_bytes'].mean() / 1024:.1f} KB\")\n",
    "print(f\"Largest page:        {df.loc[df['size_bytes'].idxmax(), 'title']} ({df['size_bytes'].max() / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Top Artists by Subpage Count ===\n",
      "root_artist\n",
      "Mafumafu          5\n",
      "Soraru            4\n",
      "Hanatan           2\n",
      "Xea               2\n",
      "After the Rain    1\n",
      "Ado               1\n",
      "3bu               1\n",
      "Alfakyun.         1\n",
      "Amatsuki          1\n",
      "Amu               1\n",
      "Araki             1\n",
      "Ayaponzu*         1\n",
      "Buzz Panda        1\n",
      "Chogakusei        1\n",
      "Chomaiyo          1\n",
      "Choumiryou        1\n",
      "Chrono Reverse    1\n",
      "Clear             1\n",
      "CleeNoah          1\n",
      "Colon             1\n"
     ]
    }
   ],
   "source": [
    "# Show artists with the most subpages (Type 2 pattern)\n",
    "subpage_counts = df[df[\"is_subpage\"]].groupby(\"root_artist\").size().sort_values(ascending=False)\n",
    "\n",
    "if len(subpage_counts) > 0:\n",
    "    print(\"=== Top Artists by Subpage Count ===\")\n",
    "    print(subpage_counts.head(20).to_string())\n",
    "else:\n",
    "    print(\"No subpages found (all pages are Type 1 monolithic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Song Entry Distribution (approximate) ===\n",
      "count    925.000000\n",
      "mean      91.043243\n",
      "std       88.301458\n",
      "min        0.000000\n",
      "25%       25.000000\n",
      "50%       68.000000\n",
      "75%      129.000000\n",
      "max      745.000000\n",
      "Name: approx_entry_count, dtype: float64\n",
      "\n",
      "Total estimated song entries: 84,215\n",
      "\n",
      "Top 10 pages by entry count:\n",
      "                                                            title root_artist  approx_entry_count\n",
      "                                                    Hanatan/Songs     Hanatan                 745\n",
      "Hanatan/Songs/Covers on YouTube Livestreams and TwitCasting Lives     Hanatan                 638\n",
      "                                                   Kanipan./Songs    Kanipan.                 558\n",
      "                                               Soraru/Songs/Cover      Soraru                 516\n",
      "                                                  Uratanuki/Songs   Uratanuki                 489\n",
      "                                                  Meramipop/Songs   Meramipop                 485\n",
      "                                                           Rachie      Rachie                 433\n",
      "                                                     Akatin/Songs      Akatin                 429\n",
      "                                                   Amatsuki/Songs    Amatsuki                 420\n",
      "                                                   Raon Lee/Songs    Raon Lee                 409\n"
     ]
    }
   ],
   "source": [
    "# Quick check: count # lines (song entries) per file\n",
    "entry_counts = []\n",
    "for _, row in df.iterrows():\n",
    "    filepath = DATA_RAW / row[\"filename\"]\n",
    "    text = filepath.read_text(encoding=\"utf-8\")\n",
    "    # Count lines starting with # (song entries inside {{Playlist}})\n",
    "    # Simple heuristic — we'll do proper extraction in notebook 02\n",
    "    count = len(re.findall(r'^\\s*#\\s*\"', text, re.MULTILINE))\n",
    "    entry_counts.append(count)\n",
    "\n",
    "df[\"approx_entry_count\"] = entry_counts\n",
    "\n",
    "print(\"=== Song Entry Distribution (approximate) ===\")\n",
    "print(df[\"approx_entry_count\"].describe())\n",
    "print(f\"\\nTotal estimated song entries: {df['approx_entry_count'].sum():,}\")\n",
    "print(f\"\\nTop 10 pages by entry count:\")\n",
    "top10 = df.nlargest(10, \"approx_entry_count\")[[\"title\", \"root_artist\", \"approx_entry_count\"]]\n",
    "print(top10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Raw data is saved in `data/raw/`. Next step: `02_exploratory_analysis.ipynb` for detailed statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
